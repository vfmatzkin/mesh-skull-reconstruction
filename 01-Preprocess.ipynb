{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Preprocess the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Rigid registration and skull extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import expanduser as eu\n",
    "from headctools.registration import register_file_folder\n",
    "from headctools.preprocessing import ct_skull, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting nii.gz images from /home/franco/Code/datasets/cq500/converted/selected/test\n",
      "Found 5 images\n",
      " No save path specified. Using default save path.\n",
      "Preprocessing {'image': '/home/franco/Code/datasets/cq500/converted/selected/test/CQ500-CT-216_CT 0.625mm.nii.gz'}\n",
      "  Registering using ANTsPy...\n",
      "    Fixed image: /home/franco/Code/datasets/cq500/converted/selected/test/CQ500-CT-312_CT PRE CONTRAST THIN.nii.gz\n",
      "    Transformation will be saved in /home/franco/Code/datasets/cq500/converted/selected/test/preprocessed_ct_to_skull/CQ500-CT-216_CT 0_reg.mat\n",
      "    Registering image\n",
      "  Intensities clipping...\n",
      "  Normalizing to [0, 1]...\n",
      "  Keeping largest CC of label image\n",
      "  Generating .stl of Marching Cubes...\n",
      "    Saved image in /home/franco/Code/datasets/cq500/converted/selected/test/preprocessed_ct_to_skull/CQ500-CT-216_CT 0.625mm.nii.gz\n",
      "  Saved images in /home/franco/Code/datasets/cq500/converted/selected/test/preprocessed_ct_to_skull\n",
      "  Deleting temporary files: ['/tmp/tmpan0oxj59.stl']\n",
      "Preprocessing {'image': '/home/franco/Code/datasets/cq500/converted/selected/test/CQ500-CT-390_CT 5mm POST CONTRAST.nii.gz'}\n",
      "  Registering using ANTsPy...\n",
      "    Fixed image: /home/franco/Code/datasets/cq500/converted/selected/test/CQ500-CT-312_CT PRE CONTRAST THIN.nii.gz\n",
      "    Transformation will be saved in /home/franco/Code/datasets/cq500/converted/selected/test/preprocessed_ct_to_skull/CQ500-CT-390_CT 5mm POST CONTRAST_reg.mat\n",
      "    Registering image\n",
      "  Intensities clipping...\n",
      "  Normalizing to [0, 1]...\n",
      "  Keeping largest CC of label image\n",
      "  Generating .stl of Marching Cubes...\n",
      "    Saved image in /home/franco/Code/datasets/cq500/converted/selected/test/preprocessed_ct_to_skull/CQ500-CT-390_CT 5mm POST CONTRAST.nii.gz\n",
      "  Saved images in /home/franco/Code/datasets/cq500/converted/selected/test/preprocessed_ct_to_skull\n",
      "  Deleting temporary files: ['/tmp/tmpan0oxj59.stl', '/tmp/tmpkw9etuq3.stl']\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/tmp/tmpan0oxj59.stl'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [9]\u001B[0m, in \u001B[0;36m<cell line: 7>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;66;03m# Registered images will be saved in a subfolder with the name of the fixed img\u001B[39;00m\n\u001B[1;32m      5\u001B[0m params \u001B[38;5;241m=\u001B[39m {\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mreg_atlas_path\u001B[39m\u001B[38;5;124m'\u001B[39m: fixed_img}  \u001B[38;5;66;03m# Non-default params\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[43mct_skull\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimgs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mparams\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/headctools/headctools/preprocessing/__init__.py:17\u001B[0m, in \u001B[0;36mct_skull\u001B[0;34m(input_path, output_path, params)\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;124;03m\"\"\" CT -> skull preprocessing\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \n\u001B[1;32m     10\u001B[0m \u001B[38;5;124;03m:param input_path: Input folder containing the CT images.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;124;03m:return:\u001B[39;00m\n\u001B[1;32m     14\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     15\u001B[0m preprocessor \u001B[38;5;241m=\u001B[39m Preprocessor(input_path, output_path, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mct_to_skull\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[1;32m     16\u001B[0m                             params\u001B[38;5;241m=\u001B[39mparams)\n\u001B[0;32m---> 17\u001B[0m \u001B[43mpreprocessor\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m preprocessor\u001B[38;5;241m.\u001B[39msave_path\n",
      "File \u001B[0;32m~/Code/headctools/headctools/preprocessing/Preprocessor.py:91\u001B[0m, in \u001B[0;36mPreprocessor.run\u001B[0;34m(self, custom_preset, u_params)\u001B[0m\n\u001B[1;32m     88\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m u_params:\n\u001B[1;32m     89\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mparams\u001B[38;5;241m.\u001B[39mupdate(u_params)  \u001B[38;5;66;03m# User params\u001B[39;00m\n\u001B[0;32m---> 91\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_preset\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpipeline\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/headctools/headctools/preprocessing/Preprocessor.py:123\u001B[0m, in \u001B[0;36mPreprocessor.run_preset\u001B[0;34m(self, pipeline)\u001B[0m\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    120\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\n\u001B[1;32m    121\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe step \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m is not supported.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    122\u001B[0m         )\n\u001B[0;32m--> 123\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave_imgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/Code/headctools/headctools/preprocessing/Preprocessor.py:281\u001B[0m, in \u001B[0;36mPreprocessor.save_imgs\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m  Deleting temporary files: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtmp_files\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    280\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m f \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtmp_files:\n\u001B[0;32m--> 281\u001B[0m     \u001B[43mos\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mremove\u001B[49m\u001B[43m(\u001B[49m\u001B[43mf\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/tmp/tmpan0oxj59.stl'"
     ]
    }
   ],
   "source": [
    "imgs = eu('~/Code/datasets/cq500/converted/selected/test')\n",
    "fixed_img = os.path.join(imgs, 'CQ500-CT-312_CT PRE CONTRAST THIN.nii.gz')\n",
    "# Registered images will be saved in a subfolder with the name of the fixed img\n",
    "\n",
    "params = {'reg_atlas_path': fixed_img}  # Non-default params\n",
    "\n",
    "ct_skull(imgs, params = params)  # Preprocess the imgs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mesh atlas construction"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Decimate meshes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'headctools.preprocessing.utils' has no attribute 'decimate_meshes'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Input \u001B[0;32mIn [7]\u001B[0m, in \u001B[0;36m<cell line: 3>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m meshes_folder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124m~/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test\u001B[39m\u001B[38;5;124m'\u001B[39m\n\u001B[1;32m      2\u001B[0m decimate_factor \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.005\u001B[39m\n\u001B[0;32m----> 3\u001B[0m saved_meshes \u001B[38;5;241m=\u001B[39m \u001B[43mutils\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecimate_meshes\u001B[49m(meshes_folder, decimate_factor)\n",
      "\u001B[0;31mAttributeError\u001B[0m: module 'headctools.preprocessing.utils' has no attribute 'decimate_meshes'"
     ]
    }
   ],
   "source": [
    "meshes_folder = '~/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test'\n",
    "decimate_factor = 0.005\n",
    "saved_meshes = utils.decimate_meshes(meshes_folder, decimate_factor)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "saved_meshes"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deformetrica"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "template_path = saved_meshes[0]\n",
    "dataset_paths = saved_meshes[1:]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import deformetrica as dfca\n",
    "\n",
    "iteration_status_dictionaries = []\n",
    "\n",
    "def estimator_callback(status_dict):\n",
    "    iteration_status_dictionaries.append(status_dict)\n",
    "    return True\n",
    "\n",
    "# instantiate a Deformetrica object\n",
    "deformetrica = dfca.Deformetrica(output_dir='output', verbosity='INFO')\n",
    "\n",
    "dataset_specifications = {\n",
    "    'dataset_filenames': [[{'skull': f}] for f in dataset_paths],\n",
    "    'subject_ids': [os.path.splitext(os.path.split(f)[1])[0] for f in dataset_paths],\n",
    "}\n",
    "template_specifications = {\n",
    "    'skull': {'deformable_object_type': 'SurfaceMesh',\n",
    "              'kernel_type': 'torch', 'kernel_width': 20.0,\n",
    "              'noise_std': 10.0,\n",
    "              'filename': template_path,\n",
    "              'attachment_type': 'current'}\n",
    "}\n",
    "estimator_options={'optimization_method_type': 'GradientAscent', 'initial_step_size': 1.,\n",
    "                   'max_iterations': 25, 'max_line_search_iterations': 10, 'callback': estimator_callback}\n",
    "\n",
    "# perform a deterministic atlas estimation\n",
    "model = deformetrica.estimate_deterministic_atlas(template_specifications, dataset_specifications,\n",
    "                                                estimator_options=estimator_options,\n",
    "                                                model_options={'deformation_kernel_type': 'torch', 'deformation_kernel_width': 40.0, 'dtype': 'float32'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decimating meshes...\n",
      "  Input folder: /home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test\n",
      "  Found 4 meshes\n",
      "    [1/4] Input mesh: CQ500-CT-54_CT Thin Plain.stl\n",
      "      Decimated mesh saved in /home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-54_CT Thin Plain_decimated_0perc_3312points.stl\n",
      "    [2/4] Input mesh: CQ500-CT-182_CT PRE CONTRAST THIN.stl\n",
      "      Decimated mesh saved in /home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-182_CT PRE CONTRAST THIN_decimated_0perc_3507points.stl\n",
      "    [3/4] Input mesh: CQ500-CT-405_CT Thin Plain.stl\n",
      "      Decimated mesh saved in /home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-405_CT Thin Plain_decimated_0perc_3425points.stl\n",
      "    [4/4] Input mesh: CQ500-CT-133_CT Plain 3mm.stl\n",
      "      Decimated mesh saved in /home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-133_CT Plain 3mm_decimated_0perc_3308points.stl\n"
     ]
    }
   ],
   "source": [
    "import vedo\n",
    "\n",
    "print('Decimating meshes...')\n",
    "decimate_factor = 0.005\n",
    "\n",
    "meshes_folder = eu('~/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test')\n",
    "print( f'  Input folder: {meshes_folder}')\n",
    "meshes_paths = [os.path.join(meshes_folder, f) for f in os.listdir(meshes_folder) if f.endswith('.stl') and 'decimated' not in f]\n",
    "saved_meshes = []\n",
    "print(f'  Found {len(meshes_paths)} meshes')\n",
    "for i, mesh in enumerate(meshes_paths):\n",
    "    print(f'    [{i+1}/{len(meshes_paths)}] Input mesh: {os.path.split(mesh)[1]}')\n",
    "    v_mesh = vedo.Mesh(mesh).decimate(decimate_factor)\n",
    "    decimated_path = mesh.replace('.stl', f'_decimated_{int(decimate_factor * 100)}perc_{v_mesh.points().shape[0]}points.stl')\n",
    "    v_mesh.write(decimated_path)\n",
    "    saved_meshes.append(decimated_path)\n",
    "    print(f'      Decimated mesh saved in {decimated_path}')\n",
    "    \n",
    "saved_meshes = [utils.stl_to_vtk(f) for f in saved_meshes]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-54_CT Thin Plain_decimated_0perc_3312points.vtk',\n",
       " '/home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-182_CT PRE CONTRAST THIN_decimated_0perc_3507points.vtk',\n",
       " '/home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-405_CT Thin Plain_decimated_0perc_3425points.vtk',\n",
       " '/home/fmatzkin/Code/datasets/cq500/converted/selected/preprocessed_ct_to_skull/test/CQ500-CT-133_CT Plain 3mm_decimated_0perc_3308points.vtk']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_meshes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Deformetrica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "template_path = saved_meshes[0]\n",
    "dataset_paths = saved_meshes[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logger has been set to: INFO\n",
      ">> No initial CP spacing given: using diffeo kernel width of 40.0\n",
      "OMP_NUM_THREADS found in environment variables. Using value OMP_NUM_THREADS=4\n",
      "context has already been set\n",
      ">> No specified state-file. By default, Deformetrica state will by saved in file: output/deformetrica-state.p.\n",
      ">> Set of 80 control points defined.\n",
      ">> Momenta initialized to zero, for 3 subjects.\n",
      ">> Started estimator: GradientAscent\n",
      "------------------------------------- Iteration: 0 -------------------------------------\n",
      ">> Log-likelihood = -1.675E+05 \t [ attachment = -1.675E+05 ; regularity = 0.000E+00 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.545E-05   and   6.471E+04 \t[ landmark_points ]\n",
      "\t\t1.183E-04   and   8.457E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 1 -------------------------------------\n",
      ">> Log-likelihood = -1.589E+05 \t [ attachment = -1.589E+05 ; regularity = -3.070E+00 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t2.318E-05   and   6.003E+04 \t[ landmark_points ]\n",
      "\t\t1.774E-04   and   7.852E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 2 -------------------------------------\n",
      ">> Log-likelihood = -1.479E+05 \t [ attachment = -1.479E+05 ; regularity = -1.750E+01 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t3.477E-05   and   5.373E+04 \t[ landmark_points ]\n",
      "\t\t2.661E-04   and   7.023E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 3 -------------------------------------\n",
      ">> Log-likelihood = -1.350E+05 \t [ attachment = -1.350E+05 ; regularity = -5.507E+01 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t5.216E-05   and   4.599E+04 \t[ landmark_points ]\n",
      "\t\t3.991E-04   and   5.969E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 4 -------------------------------------\n",
      ">> Log-likelihood = -1.215E+05 \t [ attachment = -1.214E+05 ; regularity = -1.318E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t7.823E-05   and   3.790E+04 \t[ landmark_points ]\n",
      "\t\t5.986E-04   and   4.780E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 5 -------------------------------------\n",
      ">> Log-likelihood = -1.089E+05 \t [ attachment = -1.086E+05 ; regularity = -2.622E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.174E-04   and   3.096E+04 \t[ landmark_points ]\n",
      "\t\t8.980E-04   and   3.630E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 6 -------------------------------------\n",
      ">> Log-likelihood = -9.800E+04 \t [ attachment = -9.755E+04 ; regularity = -4.496E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.760E-04   and   2.541E+04 \t[ landmark_points ]\n",
      "\t\t1.347E-03   and   2.668E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 7 -------------------------------------\n",
      ">> Log-likelihood = -8.904E+04 \t [ attachment = -8.835E+04 ; regularity = -6.846E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t2.640E-04   and   2.053E+04 \t[ landmark_points ]\n",
      "\t\t2.020E-03   and   1.969E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 8 -------------------------------------\n",
      ">> Log-likelihood = -8.135E+04 \t [ attachment = -8.039E+04 ; regularity = -9.552E+02 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t3.961E-04   and   1.643E+04 \t[ landmark_points ]\n",
      "\t\t3.031E-03   and   1.520E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 9 -------------------------------------\n",
      ">> Log-likelihood = -7.416E+04 \t [ attachment = -7.290E+04 ; regularity = -1.265E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t5.941E-04   and   1.301E+04 \t[ landmark_points ]\n",
      "\t\t4.546E-03   and   1.220E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 10 -------------------------------------\n",
      ">> Log-likelihood = -6.747E+04 \t [ attachment = -6.579E+04 ; regularity = -1.671E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t8.911E-04   and   1.267E+04 \t[ landmark_points ]\n",
      "\t\t6.819E-03   and   1.301E+03 \t[ momenta ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t4.456E-04   and   1.267E+04 \t[ landmark_points ]\n",
      "\t\t3.409E-03   and   1.301E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 11 -------------------------------------\n",
      ">> Log-likelihood = -6.595E+04 \t [ attachment = -6.419E+04 ; regularity = -1.761E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t4.456E-04   and   1.626E+04 \t[ landmark_points ]\n",
      "\t\t1.705E-03   and   1.853E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 12 -------------------------------------\n",
      ">> Log-likelihood = -6.422E+04 \t [ attachment = -6.238E+04 ; regularity = -1.841E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t4.456E-04   and   1.401E+04 \t[ landmark_points ]\n",
      "\t\t8.524E-04   and   1.333E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 13 -------------------------------------\n",
      ">> Log-likelihood = -6.289E+04 \t [ attachment = -6.102E+04 ; regularity = -1.875E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t6.683E-04   and   1.069E+04 \t[ landmark_points ]\n",
      "\t\t1.279E-03   and   1.142E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 14 -------------------------------------\n",
      ">> Log-likelihood = -6.165E+04 \t [ attachment = -5.968E+04 ; regularity = -1.975E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.003E-03   and   1.539E+04 \t[ landmark_points ]\n",
      "\t\t1.918E-03   and   1.448E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 15 -------------------------------------\n",
      ">> Log-likelihood = -6.089E+04 \t [ attachment = -5.889E+04 ; regularity = -2.007E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.003E-03   and   2.181E+04 \t[ landmark_points ]\n",
      "\t\t9.589E-04   and   2.138E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 16 -------------------------------------\n",
      ">> Log-likelihood = -5.958E+04 \t [ attachment = -5.753E+04 ; regularity = -2.052E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.003E-03   and   2.108E+04 \t[ landmark_points ]\n",
      "\t\t4.795E-04   and   1.856E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 17 -------------------------------------\n",
      ">> Log-likelihood = -5.837E+04 \t [ attachment = -5.631E+04 ; regularity = -2.062E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.504E-03   and   1.898E+04 \t[ landmark_points ]\n",
      "\t\t7.192E-04   and   1.847E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 18 -------------------------------------\n",
      ">> Log-likelihood = -5.741E+04 \t [ attachment = -5.529E+04 ; regularity = -2.125E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t7.519E-04   and   1.899E+04 \t[ landmark_points ]\n",
      "\t\t7.192E-04   and   1.723E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 19 -------------------------------------\n",
      ">> Log-likelihood = -5.651E+04 \t [ attachment = -5.437E+04 ; regularity = -2.137E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.128E-03   and   1.742E+04 \t[ landmark_points ]\n",
      "\t\t1.079E-03   and   1.731E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 20 -------------------------------------\n",
      ">> Log-likelihood = -5.594E+04 \t [ attachment = -5.376E+04 ; regularity = -2.180E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.128E-03   and   2.123E+04 \t[ landmark_points ]\n",
      "\t\t5.394E-04   and   1.875E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 21 -------------------------------------\n",
      ">> Log-likelihood = -5.557E+04 \t [ attachment = -5.338E+04 ; regularity = -2.186E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.692E-03   and   2.439E+04 \t[ landmark_points ]\n",
      "\t\t8.091E-04   and   2.306E+03 \t[ momenta ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t8.459E-04   and   2.439E+04 \t[ landmark_points ]\n",
      "\t\t4.045E-04   and   2.306E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 22 -------------------------------------\n",
      ">> Log-likelihood = -5.403E+04 \t [ attachment = -5.181E+04 ; regularity = -2.222E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t1.269E-03   and   1.682E+04 \t[ landmark_points ]\n",
      "\t\t6.068E-04   and   1.503E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 23 -------------------------------------\n",
      ">> Log-likelihood = -5.310E+04 \t [ attachment = -5.087E+04 ; regularity = -2.232E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t6.344E-04   and   1.079E+04 \t[ landmark_points ]\n",
      "\t\t6.068E-04   and   1.159E+03 \t[ momenta ]\n",
      "------------------------------------- Iteration: 24 -------------------------------------\n",
      ">> Log-likelihood = -5.242E+04 \t [ attachment = -5.015E+04 ; regularity = -2.269E+03 ]\n",
      ">> Step size and gradient norm: \n",
      "\t\t9.516E-04   and   8.698E+03 \t[ landmark_points ]\n",
      "\t\t9.102E-04   and   8.475E+02 \t[ momenta ]\n",
      "------------------------------------- Iteration: 25 -------------------------------------\n",
      ">> Log-likelihood = -5.188E+04 \t [ attachment = -4.959E+04 ; regularity = -2.296E+03 ]\n",
      ">> Estimation took: 01 minutes and 59 seconds\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import deformetrica as dfca\n",
    "\n",
    "iteration_status_dictionaries = []\n",
    "\n",
    "def estimator_callback(status_dict):\n",
    "    iteration_status_dictionaries.append(status_dict)\n",
    "    return True\n",
    "\n",
    "# instantiate a Deformetrica object\n",
    "deformetrica = dfca.Deformetrica(output_dir='output', verbosity='INFO')\n",
    "\n",
    "dataset_specifications = {\n",
    "    'dataset_filenames': [[{'skull': f}] for f in dataset_paths],\n",
    "    'subject_ids': [os.path.splitext(os.path.split(f)[1])[0] for f in dataset_paths],\n",
    "}\n",
    "template_specifications = {\n",
    "    'skull': {'deformable_object_type': 'SurfaceMesh',\n",
    "              'kernel_type': 'torch', 'kernel_width': 20.0,\n",
    "              'noise_std': 10.0,\n",
    "              'filename': template_path,\n",
    "              'attachment_type': 'current'}\n",
    "}\n",
    "estimator_options={'optimization_method_type': 'GradientAscent', 'initial_step_size': 1.,\n",
    "                   'max_iterations': 25, 'max_line_search_iterations': 10, 'callback': estimator_callback}\n",
    "\n",
    "# perform a deterministic atlas estimation\n",
    "model = deformetrica.estimate_deterministic_atlas(template_specifications, dataset_specifications,\n",
    "                                                estimator_options=estimator_options,\n",
    "                                                model_options={'deformation_kernel_type': 'torch', 'deformation_kernel_width': 40.0, 'dtype': 'float32'})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "159c917228f05416d9b819fe32ed20dc8363f68fb18cb83ee7ae4737bd8f0b32"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}